1. Gradient descent is an algorithm for finding values of parameters w and b that minimize the cost function J. 
![image](https://github.com/user-attachments/assets/907948b7-867d-4471-8e44-57b4d80a1c9a)<br>
When ![image](https://github.com/user-attachments/assets/b3c5ba2b-ff0d-4bde-906d-0ddd1c16d576) is a negative number (less than zero), what happens to w after one update step?

- [x] w increases.
- [ ] It is not possible to tell if w will increase or decrease. 
- [ ] w decreases
- [ ] w stays the same 

2. For linear regression, what is the update step for parameter b?
- [x] ![image](https://github.com/user-attachments/assets/d5bc878b-35fb-4074-a980-6bf52f1f8400)
- [ ] ![image](https://github.com/user-attachments/assets/3f9029a3-2fc9-46a0-9a54-f412cd032c12)

