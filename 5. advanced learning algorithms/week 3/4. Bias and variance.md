**Question 1**<br>
![Screenshot_47](https://github.com/user-attachments/assets/4c995b3f-64a3-4f78-be41-07208e1ef336)<br>
If the model's cross validation error J_cv is much higher than the training error J_(train), this is an indication that the model has…
- [ ] high bias
- [ ] Low bias
- [ ] Low variance
- [x] high variance

**Question 2**<br>
![Screenshot_48](https://github.com/user-attachments/assets/c5f92e94-3581-4661-a466-0cf5e9ee197d)<br>
Which of these is the best way to determine whether your model has high bias (has underfit the training data)?
- [ ] See if the training error is high (above 15% or so) 
- [ ] See if the cross validation error is high compared to the baseline level of performance 
- [x] Compare the training error to the baseline level of performance
- [ ] Compare the training error to the cross validation error.

**Question 3**<br>
![Screenshot_49](https://github.com/user-attachments/assets/2006a9e8-1acf-4c56-be60-9029f60b92d6)<br>
You find that your algorithm has high bias. Which of these seem like good options for improving the algorithm’s performance? Hint: two of these are correct. 
- [x] Decrease the regularization parameter λ (lambda)
- [ ] Collect more training examples 
- [ ] Remove examples from the training set
- [x] Collect additional features or add polynomial features 

**Question 4**<br>
You find that your algorithm has a training error of 2%, and a cross validation error of 20% (much higher than the training error). Based on the conclusion you would draw about whether the algorithm has a high bias or high variance problem, which of these seem like good options for improving the algorithm’s performance? Hint: two of these are correct. 
- [ ] Decrease the regularization parameter λ
- [ ] Reduce the training set size 
- [x] Increase the regularization parameter λ
- [x] Collect more training data
