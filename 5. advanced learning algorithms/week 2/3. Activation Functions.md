**Question 1**<br>
![Screenshot_52](https://github.com/user-attachments/assets/14d3aed7-d3bd-46bb-8b44-f72da528e0c8)<br>
Which of the following activation functions is the most common choice for the hidden layers of a neural network?
- [ ] Sigmoid
- [x] ReLU (rectified linear unit)
- [ ] Most hidden layers do not use any activation function 
- [ ] Linear

**Question 2**<br>
![Screenshot_53](https://github.com/user-attachments/assets/e9e59a1d-d6fd-4c70-81f7-5a43a65de72e)<br>
For the task of predicting housing prices, which activation functions could you choose for the output layer? Choose the 2 options that apply
- [x] linear 
- [ ] Sigmoid
- [x] ReLU 

**Question 3**<br>
True/False? A neural network with many layers but no activation function (in the hidden layers) is not effective; thatâ€™s why we should instead use the linear activation function in every hidden layer. 
- [x] False
- [ ] True
