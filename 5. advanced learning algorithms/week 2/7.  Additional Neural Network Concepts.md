**Question 1**<br>
![Screenshot_51](https://github.com/user-attachments/assets/e0d41371-5d2b-40e1-aa52-776c6def22db)<br>
The Adam optimizer is the recommended optimizer for finding the optimal parameters of the model. How do you use the Adam optimizer in TensorFlow?<br>
- [ ] The call to model.compile() uses the Adam optimizer by default
- [ ] The call to model.compile() will automatically pick the best optimizer, whether it is gradient descent, Adam or something else. So thereâ€™s no need to pick an optimizer manually. 
- [x] When calling model.compile, set optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3).
- [ ] The Adam optimizer works only with Softmax outputs. So if a neural network has a Softmax output layer, TensorFlow will automatically pick the Adam optimizer. 

**Question 2**<br>
![Screenshot_52](https://github.com/user-attachments/assets/b3aef76c-97d7-4d4d-8f99-3598ec621cd8)<br>
The lecture covered a different layer type where each single neuron of the layer does not look at all the values of the input vector that is fed into that layer. What is this name of the layer type discussed in lecture?
- [ ] A fully connected layer
- [x] convolutional layer
- [ ] Image layer
- [ ] 1D layer or 2D layer (depending on the input dimension) 
