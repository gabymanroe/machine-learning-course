**Question 1**<br>
![Screenshot_46](https://github.com/user-attachments/assets/17dce0be-ec87-4cdc-a6d4-0b5fff09a4d8)<br>
For a multiclass classification task that has 4 possible outputs, the sum of all the activations adds up to 1. For a multiclass classification task that has 3 possible outputs, the sum of all the activations should add up to ….
- [x] 1
- [ ] More than 1
- [ ] Less than 1
- [ ] It will vary, depending on the input x. 

**Question 2**<br>
![Screenshot_47](https://github.com/user-attachments/assets/08407e2c-9f90-486b-b370-bb5438f7f2fc)<br>
For multiclass classification, the cross entropy loss is used for training the model. If there are 4 possible classes for the output, and for a particular training example, the true class of the example is class 3 (y=3), then what does the cross entropy loss simplify to? [Hint: This loss should get smaller when a_3 gets larger]
- [x] ![Screenshot_49](https://github.com/user-attachments/assets/00368b35-3fdc-465c-9962-392b63335a85)
- [ ] z_3/(z_1+z_2+z_3+z_4) 
- [ ] ![Screenshot_50](https://github.com/user-attachments/assets/380d57ab-9f7b-498c-aabd-b0350a995211)
- [ ] z_3 

**Question 3**<br>
![Screenshot_48](https://github.com/user-attachments/assets/ee62da69-22af-48f9-a57d-d5e7138b11a0)<br>
For multiclass classification, the recommended way to implement softmax regression is to set from_logits=True in the loss function, and also to define the model's output layer with…
- [ ] a 'softmax' activation
- [x] a 'linear' activation
