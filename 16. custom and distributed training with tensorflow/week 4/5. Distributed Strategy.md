**Question 1**<br>
Which of the following is true about training your model using data parallelism technique? Check all that are true.
- [x] Weights from different machines are aggregated and updated into a single model.
- [x] The full data set is split up and subsets of the data are stored across multiple machines
- [ ] The same model architectures are used on different machines, and each machine processes the entire data set.
- [ ] All of the data is on 1 master machine, and copies of the data are then distributed to machines having different model architectures based on their capacity of processing the data.

**Question 2**<br>
In TensorFlow version 2, tf.distribute.Strategy class supports _______. Check all that apply.
- [x] Graph Mode
- [x] Eager Mode

**Question 3**<br>
Which of the following are true of both MirroredStrategy and TPU Strategy? Check all that are true.
- [ ] Uses multiple machines
- [x] Uses a single machine
- [x] The same model is replicated on each core.
- [x] Variables are synchronized (mirrored) across each replica of the model

**Question 4**<br>
To modify training code to work with Mirrored Strategy, which of the following should we do? Choose all that apply.
- [x] Adjust the batch size to equal the batch size per replica times the number of replicas
- [ ] Increase the batch size as long as the number is 2^n (e.g. 64, 128, 256 etc).
- [x] Put code that creates the model object inside the scope of “with strategy.scope()”.
- [ ] Put the code that creates, compiles and fits the model inside the scope of “with strategy.scope()”.

**Question 5**<br>
To modify training code to work with distributed data, which of the following should we do? Choose all that apply.
- [x] Use strategy.run to run the code that updates the model weights (calculating loss, calculating the gradients, and applying the gradients).
- [x] Use strategy.reduce to aggregate the losses across the replicas.
- [x] Use strategy.experimental_distribute_dataset to convert training and test sets into distributed datasets.
- [ ] Replace the code that updates the model weights (calculating loss, calculating gradients, and applying the gradients) so that each training step handles all replicas at once.

**Question 6**<br>
To use the TPU strategy, there are some steps that you’ll take before running the training code. Please think about which line of code implements each step and choose the set of code that performs these steps in this order.<br>
1. Get the TPU address
2. Find the TPU cluster
3. Connect to the TPU cluster
4. Initialize the TPU cluster
5. Create your TPU strategy
- [x] tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']<br>
      tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)<br>
      tf.config.experimental_connect_to_cluster(tpu)<br>
      tf.tpu.experimental.initialize_tpu_system(tpu)<br>
      strategy = tf.distribute.experimental.TPUStrategy(tpu)
      
- [ ] tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']<br>
      tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)<br>
      tf.config.experimental_connect_to_cluster(tpu)<br>
      tf.tpu.experimental.initialize_tpu_system(tpu)<br>
      strategy = tf.distribute.experimental.MirroredStrategy(tpu)

- [ ] tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']<br>
      tpu = tf.config.experimental_connect_to_cluster(tpu_address)<br>
      tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)<br>
      tf.tpu.experimental.initialize_tpu_system(tpu)<br>
      strategy = tf.distribute.experimental.TPUStrategy(tpu)

- [ ] strategy = tf.distribute.experimental.TPUStrategy(tpu)<br>
      tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']<br>
      tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)<br>
      tf.config.experimental_connect_to_cluster(tpu)<br>
      tf.tpu.experimental.initialize_tpu_system(tpu)
