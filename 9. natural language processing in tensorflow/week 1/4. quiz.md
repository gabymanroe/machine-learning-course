**Question 1**<br>
In the lectures, what is the name of the layer used to generate the vocabulary?
- [ ] WordTokenizer
- [ ] TextTokenizer
- [x] TextVectorization
- [ ] Tokenizer

**Question 2**<br>
Once you have generated a vocabulary, how do you encode a string sentence to an integer sequence?
- [x] Pass the string to the adapted TextVectorization layer.
- [ ] Use the texts_to_sequences() method of the adapted TextVectorization layer.
- [ ] Pass the string to the get_vocabulary() method.
- [ ] Use the texts_to_tokens() method of the adapted TextVectorization layer.

**Question 3**<br>
If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?
- [ ] Make sure that they are all the same length using the pad_sequences method of the TextVectorization layer
- [ ] Process them on the input layer of the Neural Network using the pad_sequences property
- [x] Use the pad_sequences function from tf.keras.utils
- [ ] Specify the input layer of the Neural Network to expect different sizes with dynamic_length

**Question 4**<br>
What happens at encoding when passing a string that is not part of the vocabulary?
- [ ] The word is replaced by the most common token
- [ ] The word isn’t encoded, and the sequencing ends
- [x] An out-of-vocabulary token is used to represent it.
- [ ] The word isn’t encoded, and is replaced by a zero in the sequence

**Question 5**<br>
When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?
- [ ] Pass padding=’after’ to pad_sequences when initializing it
- [ ] Call the padding method of the pad_sequences object, passing it ‘after’
- [ ] Call the padding method of the pad_sequences object, passing it ‘post’
- [x] Pass padding=’post’ to pad_sequences when initializing it

**Question 6**<br>
What's one way to convert a list of strings named 'sentences' to integer sequences? Assume you adapted a TextVectorization layer and assigned it to a variable named 'vectorize_layer'.
- [ ] vectorize_layer.fit_to_text(sentences)
- [x] vectorize_layer(sentences)
- [ ] vectorize_layer.tokenize(sentences)
- [ ] vectorize_layer.fit(sentences)

**Question 7**<br>
If you have a number of sequences of different length, and call pad_sequences on them, what’s the default result?
- [ ] Nothing, they’ll remain unchanged
- [ ] They’ll get padded to the length of the longest sequence by adding zeros to the end of shorter ones
- [x] They’ll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones
- [ ] They’ll get cropped to the length of the shortest sequence

**Question 8**<br>
Using the default settings, how does the TextVectorization standardize the string inputs?
- [ ] By lowercasing the strings.
- [ ] By stripping punctuation.
- [x] By lowercasing and stripping punctuation.
- [ ] By arranging the strings in alphabetical order.
