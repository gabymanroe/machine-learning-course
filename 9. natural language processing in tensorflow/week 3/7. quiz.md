**Question 1**<br>
When stacking LSTMs, how do you instruct an LSTM to feed the next one in the sequence?
- [ ] Ensure that return_sequences is set to True on all units
- [x] Ensure that return_sequences is set to True only on units that feed to another LSTM
- [ ] Ensure that they have the same number of units
- [ ] Do nothing, TensorFlow handles this automatically

**Question 2**<br>
How does an LSTM help understand meaning when words that qualify each other aren’t necessarily beside each other in a sentence?
- [x] Values from earlier words can be carried to later ones via a cell state
- [ ] They don’t
- [ ] They load all words into a cell state
- [ ] They shuffle the words randomly

**Question 3**<br>
What’s the best way to avoid overfitting in NLP datasets?
- [ ] Use LSTMs
- [ ] Use GRUs
- [ ] Use Conv1D
- [x] None of the above

**Question 4**<br>
What keras layer type allows LSTMs to look forward and backward in a sentence?
- [ ] Bilateral
- [x] Bidirectional
- [ ] Bothdirection
- [ ] Unilateral

**Question 5**<br>
Why does sequence make a large difference when determining semantics of language?
- [x] Because the order in which words appear dictate their impact on the meaning of the sentence
- [ ] It doesn’t
- [ ] Because the order of words doesn’t matter
- [ ] Because the order in which words appear dictate their meaning

**Question 6**<br>
How do Recurrent Neural Networks help you understand the impact of sequence on meaning?
- [ ] They shuffle the words evenly
- [x] They carry meaning from one cell to the next
- [ ] They don’t
- [ ] They look at the whole sentence at a time

**Question 7**<br>
What’s the output shape of a bidirectional LSTM layer with 64 units?
- [x] (None, 128)
- [ ] (128,1)
- [ ] (128,None)
- [ ] (None, 64)

**Question 8**<br>
If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernel size of 5 is passed over it, what’s the output shape?
- [ ] (None, 116, 124)
- [x] (None, 116, 128)
- [ ] (None, 120, 128)
- [ ] (None, 120, 124)
