**Question 1**<br>
What platforms are supported by TensorFlow Lite (Check all that apply)
- [x] Android
- [ ] Windows Phone
- [x] iOS
- [x] Some Microcontrollers
- [x] Raspberry Pi 

**Question 2**<br>
What is Quantization?
- [ ] A technique that increases precision to ensure your model works better on mobile
- [x] A technique that reduces precision and model size to work better on mobile
- [ ] A technique to optimize the size of a model for the memory map of a mobile device
- [ ] A technique to ensure compatibility across all supported platforms

**Question 3**<br>
The TFLite file format is an example of what?
- [ ] A checkpoint
- [ ] A concrete function
- [ ] A savedmodel
- [x] A flatbuffer

**Question 4**<br>
Which types of input does the TF Lite Convertor API Accept (Check all that apply)
- [x] A set of concrete functions
- [ ] A list of checkpoints
- [x] A SavedModel
- [ ] A model object
- [x] A Keras HDF5 file

**Question 5**<br>
True or False: The SavedModel format supports model Versioning
- [x] True
- [ ] False

**Question 6**<br>
If I want to save an existing Keras model, what’s the API signature:
- [ ] tf.save(model, path)
- [ ] Tf.model.save(path)
- [x] tf.saved_model.save(model, path)
- [ ] tf.saved_model.path=path

**Question 7**<br>
If I want to use the TensorFlow Lite Convertor to convert a saved model to TF Lite, what’s the API signature?
- [x] converter = tf.lite.TFLiteConverter.from_saved_model(path)<br>
      newModel = converter.convert()

- [ ] newModel = tf.lite.TFLiteConverter.fromModel(myModel).convert()

- [ ] converter = tf.lite.TFLiteConverter.convert()<br>
      newModel = converter.Convert(model_path)

- [ ] newModel = tf.lite.TFLiteConverter.convert(model_path)

**Question 8**<br>
If I have a keras model and want to convert it, what’s the method signature on TFLiteConverter
- [ ] convert_keras_model(model)
- [ ] from_keras(model)
- [ ] convert(model)
- [x] from_keras_model(model)

**Question 9**<br>
If I want to convert using a command line tool, what’s the name of the tool?
- [ ] Tflite_to_model
- [ ] tf_convert_lite
- [x] tflite_convert
- [ ] tfliteconvert

**Question 10**<br>
If I want to do post training quantization, what are the optimization options available (check all that apply)
- [ ] OPTIMIZE_FOR_ANDROID
- [x] OPTIMIZE_FOR_SIZE
- [x] OPTIMIZE_FOR_LATENCY
- [ ] OPTIMIZE_FOR_IOS
- [ ] OPTIMIZE_FOR_PERFORMANCE
