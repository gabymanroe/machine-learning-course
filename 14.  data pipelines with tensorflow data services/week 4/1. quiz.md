**Question 1**<br>
What does the acronym ETL stand for?
- [ ] External / Transform / Load
- [x] Extract / Transform / Load
- [ ] Extract / Transfer / Load
- [ ] Enhance / Transfer / Load

**Question 2**<br>
You have a multi processor machine, containing a CPU and GPU. How would you typically distribute these for training a model?
- [ ] Use CPU and GPU for all tasks in parallel
- [ ] Use CPU for extracting and the GPU for transforming and loading
- [x] Use CPU for extracting, transferring and loading, and the GPU for training
- [ ] Use CPU for extracting and loading, and the GPU for transforming

**Question 3**<br>
One way to speed up ETL is to use a cache. What’s the API for this called?
- [ ] tf.data.Dataset.ETLCache() 
- [x] tf.data.Dataset.cache()
- [ ] tf.data.Dataset.datacache()
- [ ] tf.data.DataCache()

**Question 4**<br>
I have a dataset loaded using this code: 
```
dataset = tfds.load('cats_vs_dogs',split=tfds.Split.TRAIN)
```
How would I cache it on disk? 
- [x] train_dataset = dataset.cache(filename='cache')
- [ ] train_dataset = dataset.cache()
- [ ] train_dataset = dataset.cache(cachename=file)
- [ ] train_dataset = dataset.cache(file='cache')

**Question 5**<br>
I have a dataset loaded using this code: 
```
dataset = tfds.load('cats_vs_dogs',split=tfds.Split.TRAIN)
```
How would I cache it in memory?
- [ ] train_dataset = dataset.cache(cachename='memory')  
- [x] train_dataset = dataset.cache()
- [ ] train_dataset = dataset.memorycache()
- [ ] train_dataset = dataset.cache_in_memory()

**Question 6**<br>
If I create a function called ‘augment’ that transforms data, what code would I use to apply this after loading a dataset with 
```
dataset = tfds.load('cats_vs_dogs',split=tfds.Split.TRAIN)
```
- [x] augmented_dataset = dataset.map(augment)
- [ ] augmented_dataset = dataset.augment()
- [ ] augmented_dataset = map(augment)
- [ ] augmented_dataset = dataset.augment(dataset)

**Question 7**<br>
If you want to parallelise the transform of a dataset across multiple cores, what’s the correct call?
- [x] s = dataset.map(augment, num_parallel_calls=2)
- [ ] s = dataset.map(augment, num_parallel=2)
- [ ] s = dataset.map(augment, parallel_calls=2)
- [ ] s = dataset.map(augment, 2)

**Question 8**<br>
If you’re not sure how many cores are accessible, for example, if you’re running in a shared cloud environment, how can you find out how many are available to you?
- [x] num_cores = multiprocessing.cpu_count()
- [ ] It’s not possible
- [ ] num_cores = multiprocessing.available_cpus()
- [ ] num_cores = multiprocessing.cpu.count()

**Question 9**<br>
The process of executing a custom map function over a batch of inputs is called:
- [ ] Batch mapping
- [x] Vectorization
- [ ] Map batching
- [ ] Visualization
