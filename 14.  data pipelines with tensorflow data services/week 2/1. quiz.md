**Question 1**<br>
If you want to merge two splits 'train' and 'test' together using Splits API, how would you be able to do so?
- [ ] tfds.load('mnist', split = pd.concat('train', 'test'))
- [x] tfds.load('mnist', split = 'train + test')
- [ ] tfds.load('mnist',split = np.concat('train+test'))
- [ ] tfds.load('mnist', merge = 'train+test')

**Question 2**<br>
The MNISTv3 dataset supports the Splits API. The train split has 70000 records in it. If you just want to create a subsplit of the first 7000 records and want to use the python slicing notation instead of Splits API, what would be the answer?
- [ ] tfds.load('mnist:3.*.*', subsplit='train[:7000]')
- [ ] Read the entire train split, create a new dataset, iterate over the first 7000 of the 70000, and copy the records one-by-one to the new dataset.
- [x] tfds.load('mnist:3.*.*', split='train[:7000]')
- [ ] tfds.load('mnist:3.*.*', split='train[7000:]')

**Question 3**<br>
If you want a subsplit of the first 10% of the MNISTv3 training records, what would the code look like using the Splits API? 
- [ ] tfds.load('mnist:3.*.*', subsplit='train[:10%]')
- [x] tfds.load('mnist:3.*.*', split='train[:10%]')
- [ ] tfds.load('mnist:3.*.*', split='train[10%:]')
- [ ] tfds.load('mnist:3.*.*', subsplit='train[10%:]') 

**Question 4**<br>
How many validation splits will this code generate?
```
 val_ds = tfds.load('mnist:3.*.*', split = ['train[{}%:{}%]'.format(int(k/4),int((k+40)/4)) for k in range(0,400,40)])
```
- [ ] 40
- [x] 10
- [ ] Will throw an Error
- [ ] 5

**Question 5**<br>
True or False : The TFRecord shards are only created for the training data.
- [x] False
- [ ] True

**Question 6**<br>
Which of the following could be used to investigate how the TFRecords files look like? 
- [ ] filename = "your_tf_record_file"raw_file = tf.RecordDataset(filename)for raw_record in raw_file.take(1): print(repr(raw_record)) 
- [ ] filename = "your_tf_record_file"raw_file = tf.keras.dataset.load(filename)for raw_record in raw_file.take(1): print(repr(raw_record))  
- [x] filename = "your_tf_record_file"raw_file = tf.data.TFRecordDataset(filename)for raw_record in raw_file.take(1): print(repr(raw_record))  
- [ ] filename = "your_tf_record_file"raw_file = tf.loads(filename)for raw_record in raw_file.take(1): print(repr(raw_record))  

**Question 7**<br>
Below is an example of how raw TFRecord files look like when you properly read and print them with TFRecordDataset methods.<br>
![image](https://github.com/user-attachments/assets/1ffd7227-8c2e-450e-a99d-d6c8e2c13b78)<br>
To parse them into proper format, which of the following options need to be implemented?
- [x] Creating a feature description dictionary
- [ ] Apply the parsing function to each item in the dataset using thekeras.dataset.map method
- [ ] Creating a parsing function using tfds.load()
- [x] Creating a parsing function using tf.io.parse_single_example()
- [x] Apply the parsing function to each item in the dataset using thetf.data.Dataset.mapmethod. 
